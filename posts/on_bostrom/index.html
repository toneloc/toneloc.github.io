<!DOCTYPE html>
<html lang='en' dir='auto'><head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='I take issue here with the notion of presentient and nearly deific artificial intelligences who shall, it is supposed, be programmed to achieve at all costs a &ldquo;supergoal&rdquo; &hellip; only later to wreak accidental havoc on their hapless human inventors.
Consider a specific incarnation of the idea: the meme of the paper clip-maximizing super AI, popularized by Oxford University&rsquo;s Prof. Nick Bostrom. It &ldquo;seems perfectly possible,&rdquo; writes Bostrom, &ldquo;to have a superintelligence whose sole goal is something completely arbitrary, such as to manufacture as many paperclips as possible, and who would resist with all its might any attempt to alter this goal.'>
<meta name='theme-color' content='#ffcd00'>

<meta property='og:title' content='The Hidden Fallacy of Paper Clip Maximizing-Robots and Supergoals • Tony Klausing'>
<meta property='og:description' content='I take issue here with the notion of presentient and nearly deific artificial intelligences who shall, it is supposed, be programmed to achieve at all costs a &ldquo;supergoal&rdquo; &hellip; only later to wreak accidental havoc on their hapless human inventors.
Consider a specific incarnation of the idea: the meme of the paper clip-maximizing super AI, popularized by Oxford University&rsquo;s Prof. Nick Bostrom. It &ldquo;seems perfectly possible,&rdquo; writes Bostrom, &ldquo;to have a superintelligence whose sole goal is something completely arbitrary, such as to manufacture as many paperclips as possible, and who would resist with all its might any attempt to alter this goal.'>
<meta property='og:url' content='/posts/on_bostrom/'>
<meta property='og:site_name' content='Tony Klausing'>
<meta property='og:type' content='article'><meta property='article:section' content='posts'><meta property='article:published_time' content='2017-10-15T00:00:00Z'/><meta property='article:modified_time' content='2017-10-15T00:00:00Z'/><meta name='twitter:card' content='summary'>

<meta name="generator" content="Hugo 0.72.0" />

  <title>The Hidden Fallacy of Paper Clip Maximizing-Robots and Supergoals • Tony Klausing</title>
  <link rel='canonical' href='/posts/on_bostrom/'>
  
  
  <link rel='icon' href='/favicon.ico'>
<link rel='stylesheet' href='/assets/css/main.ab98e12b.css'><link rel='stylesheet' href='/css/custom.css'><link rel='stylesheet' href='/css/prism.css'><style>
:root{--color-accent:#ffcd00;}
</style>

  

</head>
<body class='page type-posts'>

  <div class='site'><a class='screen-reader-text' href='#content'>Skip to Content</a><div class='main'><nav id='main-menu' class='menu main-menu' aria-label='Main Menu'>
  <div class='container'>
    
    <ul><li class='item'>
        <a href='/'>About</a>
      </li><li class='item'>
        <a href='/posts'>Blog</a>
      </li></ul>
  </div>
</nav><div class='header-widgets'>
        <div class='container'></div>
      </div>

      <header id='header' class='header site-header'>
        <div class='container sep-after'>
          <div class='header-info'><p class='site-title title'>Tony Klausing</p><p class='desc site-desc'></p>
          </div>
        </div>
      </header>

      <main id='content'>


<article lang='en' class='entry'>
  <header class='header entry-header'>
  <div class='container sep-after'>
    <div class='header-info'>
      <h1 class='title'>The Hidden Fallacy of Paper Clip Maximizing-Robots and Supergoals</h1>
      

    </div>
    <div class='entry-meta'>
  <span class='posted-on'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"/>
  <line x1="16" y1="2" x2="16" y2="6"/>
  <line x1="8" y1="2" x2="8" y2="6"/>
  <line x1="3" y1="10" x2="21" y2="10"/>
  
</svg>
<span class='screen-reader-text'>Posted on </span>
  <time class='entry-date' datetime='2017-10-15T00:00:00Z'>2017, Oct 15</time>
</span>

  
  

</div>


  </div>
</header>

  
  

  <div class='container entry-content'>
  <p>I take issue here with the notion of presentient and nearly deific artificial intelligences who shall, it is supposed, be programmed to achieve at all costs a &ldquo;supergoal&rdquo; &hellip; only later to wreak accidental havoc on their hapless human inventors.</p>
<p>Consider a specific incarnation of the idea: the meme of the paper clip-maximizing super AI, popularized by Oxford University&rsquo;s Prof. Nick Bostrom. It &ldquo;seems perfectly possible,&rdquo; writes Bostrom, &ldquo;to have a superintelligence whose sole goal is something completely arbitrary, such as to manufacture as many paperclips as possible, and who would resist with all its might any attempt to alter this goal.&rdquo; [1]</p>
<p>Let your imagination roam wild with worst-case results of the paper clip thought experiment: Railroads shorn from the Earth for their iron. Skyscrapers dismantled by nanorobots for more paper clips. And, just before the credits roll, hordes of humans enslaved in iron ore mines, toiling in the shadows of Martian paper clip mountains. The idea is compelling in its glossy SyFy simplicity, even resurrected in late 2017 with the creation of a viral internet game based on the idea (You can play it <a href="http://www.decisionproblem.com/paperclips/index2.html">here</a>). [2]</p>
<p>Bostrom expounds upon these ideas in his 2011 book <em>Superintelligence</em> and earlier in a 2003 essay. [2] [3] There is also considerable overlap here with Singularity soothsayers like Ray Kurzweil and Elezier Yudowsky, who may have coined the term &ldquo;supergoal&rdquo; (as applied to AI) in a 2001 blog post. [4]</p>
<p>Yet Bostrom asserts what few would deny: that if humans were capable of rendering an invention of such strategic might, then it could cause unintended harm. &ldquo;For better or worse,&rdquo; laments Bostrom, &ldquo;artificial intellects need not share our human motivational tendencies.&rdquo; But who would disagree? Here&rsquo;s your friendly reminder, Prof. Bostrom, that all seven billion humans have different &ldquo;motivational tendencies&rdquo;. Furthermore, it&rsquo;s not surprising to me that if Office Depot is capable of cranking out ravenous paper clip-making Franken-robots, then we may be in big trouble. What&rsquo;s Lockheed-Martin doing?</p>
<p>Bostrom has a plan, presented as it seems with all due earnest: &ldquo;It seems that the best way to ensure that a superintelligence will have a beneficial impact on the world is to endow it with philanthropic values.&rdquo; Amen, Prof. Bostrom. This charitable suggestion in fact sounds like a good undergraduate computer science assignment:</p>
<ol>
<li><em>Code up the virtues of faith, hope, and love.</em></li>
<li><em>Assign them to this paper clip-maximizing robot.</em></li>
<li><em>Submissions will be judged by a panel of philsophers, monks, and rabbis.</em></li>
</ol>
<p>Philanthropy, as it turns out, actually isn&rsquo;t so easy to define and implement. Ask Bill Gate or the Red Cross. One can have the best interests of someone else in mind but still screw it up, perhaps through no fault of their own. And philanthropic outcomes are not one-dimensional. Some parties may be pleased with the result; some may in turn be revolted.</p>
<p>The interesting point, though, that I want to take care to point out, is that programming a philanthropic supergoal is only slighlty more absurd that the very concept of being able to program a paper clip maximizing supergoal in the first place.</p>
<p>Programming a paper clip maximier, would be, when you think about, really hard, and, in fact, &ldquo;perfectly impossible&rdquo;. For example:</p>
<ul>
<li>Should the robot keep humans around or put them to work?</li>
<li>Instead of ripping out railroads to make paper clips, should the AI keep railroad lines for transportation?</li>
<li>What precise type of paper clip is acceptable? What materials, what maximum and minimum angles and dimensions, how lustrous, how dense, how tensile, how hard?</li>
<li>On what time horizon shall we optimize paper clip production? Ought the robot binge on paper clip production for 10 years, leaving the Earth ruined? Or should the robot promote &ldquo;sustainable&rdquo; paper clip production, optimizing for a longer time horizon?</li>
<li>Other planets in our solar system, and the Earth&rsquo;s core, have considerable amounts of metals such as nickel, iron, and aluminum. Should the AI invest in R&amp;D to harvest hard-to-get and interplanetary metals? What about intergalactic travel? And on what time horizon should these investments play out?</li>
</ul>
<p>These questions and calculations involve setting parameters and heuristics and balancing risks about an unpredictable future, which any creature short of Laplace&rsquo;s demon or Yahweh will find pretty tough to resolve. How the AI should balance unpredicatble risks and make decisions in the face of unrelenting uncertainty (often termed <em>Knightian uncertainty</em>) is the golden question.</p>
<p>But it&rsquo;s not just the golden question for computer science-y futurists; it&rsquo;s the golden question full stop, the <em>raison d&rsquo;être</em> of existence: what should we do? The robot can’t answer these questions, because, in fact, there is no answer. The best way to maximize paper clip production is not an answer for which we can design in 2017, implement by 2020, and have finished by 2023.</p>
<p>The calculation problem will prove just as intractable for any artificial intelligence as it is for any biological intelligence. Presenting the world as a closed system, as what a Samuelsonian economist may term a  &ldquo;constrained maximization problem&rdquo;, is wrong, both in technological and economic forecasting. A robot&rsquo;s utility curve, actually, is just as fictional as a human&rsquo;s utility curve.</p>
<p>So in the end, Bostrom&rsquo;s scenario is silly on two fronts. First, if he is merely pointing out that software and hardware can have bugs &hellip; well, yeah, we get it. Anything set in motion by humans can have drastic and unforeseen consequences. In addition to the paper clip maximizer, notable fictional missteps include Frankenstein&rsquo;s monster, <em>Dr. Strangelove</em>&rsquo;s Doomsday device, and <em>2001: A Space Odyssey</em>&rsquo;s HAL 9000. We should all remain aware, as Prof. Bostrom does well, that &ldquo;The best-laid plans of mice and men / Go oft awry&rdquo;.</p>
<p>In a related error, however, Bostrom seems to allude to the impossible: that optimizing functions can somehow predict the future in some grandly orchestrated master plan beyond our comprehension. This is not futuristic science; it is fiction. AI is limited &ndash; like humans, like aliens, and like everything under the sun of physics-as-we-know-it &ndash; by an all-encompassing uncertainty.</p>
<p>So, yes, we can definitely screw something up with technological invention, but let&rsquo;s not yet suppose that we can screw it by inventing a godlike machine not beholden to physics and space-time.</p>
<hr>
<p><strong>Citations</strong></p>
<p>[1] Bostrom, Nic. &ldquo;Ethical Issues in Advanced Artificial Intelligence.&rdquo; 2003. Accessed October 14, 2017. <a href="https://nickbostrom.com/ethics/ai.html#_ftn2">https://nickbostrom.com/ethics/ai.html#_ftn2</a></p>
<p>[2] As reported in <em>AV Club</em> <a href="https://www.avclub.com/this-game-about-watching-a-computer-make-paperclips-sur-1819366023">https://www.avclub.com/this-game-about-watching-a-computer-make-paperclips-sur-1819366023</a>.</p>
<p>[3] Bostrom, Nick. <em>Superintelligence: Paths, Dangers, Strategies</em>. Oxford, United Kingdom: Oxford University Press, 2011.</p>
<p>[4] Yudowski, Elezier. &ldquo;What is Friendly AI?&rdquo; 2001. Accessed October 14, 2017. <a href="http://www.kurzweilai.net/what-is-friendly-ai">http://www.kurzweilai.net/what-is-friendly-ai</a>.</p>
<p>See also:</p>
<ul>
<li><a href="https://www.nrl.navy.mil/itd/aic/sites/www.nrl.navy.mil.itd.aic/files/pdfs/altmann.goals02.pdf">Navy sponsored study of memory and goals</a></li>
<li><a href="https://intelligence.org/files/CFAI.pdf">Yudowsky attempts to define types of goals</a></li>
<li><a href="https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom">New Yorker profile on Bostrom</a></li>
</ul>
<p>n.b.</p>
<ul>
<li>The concept of &ldquo;supergoals&rdquo; borders on what Nick Szabo calls a small game fallacy, one of the strongest intellectual guards against techno-utopianism. It&rsquo;s well worth reading about the small game fallacy with advanced AI in mind - <a href="http://unenumerated.blogspot.com/2015/05/small-game-fallacies.html">http://unenumerated.blogspot.com/2015/05/small-game-fallacies.html</a></li>
<li>I tend to side with skeptics such as such as Jaron Lanier who posit that thinkers like Bostrom have a way of “dramatizing their beliefs with an end-of-days scenario” that takes the form of curious religiosity. Despite my criticism above, I welcome their thought experiments and think the line of discourse is valuable, when put in proper perspective and when the logical leaps of faith are properly identified, acknowledging that some religiosity about a certain subject may not always be wrong.</li>
</ul>

</div>

  
<footer class='entry-footer'>
  <div class='container sep-before'><div class='categories'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M22,19a2,2,0,0,1-2,2H4a2,2,0,0,1-2-2V5A2,2,0,0,1,4,3H9l2,3h9a2,2,0,0,1,2,2Z"/>
  
</svg>
<span class='screen-reader-text'>Categories: </span><a class='category' href='/categories/miscellany/'>Miscellany</a></div>

  </div>
</footer>


</article>

<nav class='entry-nav'>
  <div class='container'><div class='prev-entry sep-before'>
      <a href='/posts/welcome/'>
        <span aria-hidden='true'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="20" y1="12" x2="4" y2="12"/>
  <polyline points="10 18 4 12 10 6"/>
  
</svg>
 Previous</span>
        <span class='screen-reader-text'>Previous post: </span>Good evening, and welcome</a>
    </div><div class='next-entry sep-before'>
      <a href='/posts/no_sports/'>
        <span class='screen-reader-text'>Next post: </span>No Sports<span aria-hidden='true'>Next <svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="4" y1="12" x2="20" y2="12"/>
  <polyline points="14 6 20 12 14 18"/>
  
</svg>
</span>
      </a>
    </div></div>
</nav>




      </main>

      <footer id='footer' class='footer'>
        <div class='container sep-before'><div class='copyright'>
  <p></p>
</div>

        </div>
      </footer>

    </div>
  </div><script>window.__assets_js_src="/assets/js/"</script>

<script src='/assets/js/main.c3bcf2df.js'></script><script src='/js/custom.js,js/prism.js'></script>

</body>

</html>

